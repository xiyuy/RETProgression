# Base configuration for all experiments
distributed: 
  world_size: 1 # Maximum number of GPUs to use
  backend: nccl # Backend to use for distributed training

exp: 
  num_epochs: 8 # Increased from 5 to 8 epochs for better convergence
  checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_GPU1_experiment"
  checkpoint_name: null # "epoch_7.pth"
  checkpoint_freq: 1 # Save checkpoints every N epochs
  experiment_name: "baseline" # Experiment identifier (change for each experiment)
  visualization_dir: "/home/yang.xiyu/Github/RETProgression/src/visualizations" # Directory for plots

data: 
  data_dir: "/work/retprogression"
  annotations_file_name: "referable_img_grades_"
  class_labels: "referable_binary_DR"
  batch_size: 64
  shuffle: True
  num_workers: 7
  resolution: 384  # New parameter for image resolution (default was 224)
  augmentation:
    enabled: True
    strength: "moderate" # Options: "none", "moderate", "strong" 
    visualize: True # Whether to save augmentation visualizations

caching: 
  enabled: True
  train_cache_size: 1000
  val_cache_size: 1000
  prefetch_factor: 2 # Increased prefetch factor

model: 
  name: vit_base_patch16_224
  num_classes: 2
  pretrained: True

criterion: 
  type: "cross_entropy" # Options: "cross_entropy", "focal_loss"
  use_class_weights: True # Whether to use class weighting (for CrossEntropy)
  class_weights: [1.0, 13.0] # Class weights for [negative, positive]
  # Focal Loss parameters
  alpha: 0.25 # Alpha parameter for focal loss (higher values focus more on rare class)
  gamma: 2.0 # Gamma parameter for focal loss (higher values focus more on hard examples)

optimizer: 
  name: adam
  lr: 0.00005 # Learning rate
  weight_decay: 0.001
  momentum: 0.9

lr_scheduler: 
  type: "step" # Options: "step", "cosine"
  step_size: 7
  gamma: 0.1
  warmup:
    enabled: False # Whether to use learning rate warmup
    warmup_proportion: 0.1 # Proportion of first epoch for warmup
    min_lr: 1.0e-7 # Starting learning rate for warmup

optimization: 
  amp_enabled: True # Enable automatic mixed precision
  gradient_clipping: 1.0 # Max gradient norm for gradient clipping
  gradient_accumulation_steps: 2 # Accumulate gradients to reduce communication
  sync_bn: False # Synchronize batch normalization stats across GPUs
  drop_last: True # Drop incomplete batches to avoid small batch issues

# Early stopping configuration
early_stopping: 
  enabled: True
  patience: 5 # Number of epochs with no improvement after which training will be stopped
  metric: balanced_accuracy # Metric to monitor for early stopping
  mode: max # Whether to maximize or minimize the monitored metric
  min_delta: 0.001 # Minimum change to qualify as an improvement

# Profiling disabled to avoid PyTorch internal error
profiling: 
  enabled: False

# Experiment-specific configurations
# ===================================

# Experiment 1: Moderate Augmentation + Focal Loss
experiment_focal_loss:
  exp:
    experiment_name: "focal_loss"
    checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_GPU1_Iteration3_focal_loss"
  data:
    augmentation:
      strength: "moderate"
  criterion:
    type: "focal_loss"
    use_class_weights: False # Not needed for Focal Loss
    alpha: 0.25
    gamma: 2.0
  lr_scheduler:
    warmup:
      enabled: False

# Experiment 2: Moderate Augmentation + Learning Rate Warmup
experiment_warmup: 
  exp:
    num_epochs: 15  # Default: 8
    experiment_name: "warmup"
    checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_GPU1_Iter4_balanced_warmup_cosine"
  data:
    augmentation:
      strength: "moderate"
  criterion:
    type: "cross_entropy"
    use_class_weights: True
  lr_scheduler:
    type: "cosine"  # Default: "step"
    warmup:
      enabled: True
      warmup_proportion: 0.1
      min_lr: 1.0e-7

# Experiment 3: Moderate Augmentation + Focal Loss + Learning Rate Warmup
experiment_focal_warmup:
  exp:
    num_epochs: 15  # Default: 8
    experiment_name: "focal_warmup"
    checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_GPU1_Iteration3_focal_warmup"
  data:
    augmentation:
      strength: "moderate"
  criterion:
    type: "focal_loss"
    use_class_weights: False
    alpha: 1.0
    gamma: 2.0
  lr_scheduler:
    warmup:
      enabled: True
      warmup_proportion: 0.1
      min_lr: 1.0e-7

# Experiment 4: Higher Resolution with Warmup
experiment_high_resolution:
  exp:
    experiment_name: "high_resolution_warmup"
    checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_GPU1_Iteration5_high_resolution_warmup"
    num_epochs: 15  # Increased from default 8 to 15 like in warmup experiment
  data:
    resolution: 384  # Higher resolution for this experiment
    augmentation:
      strength: "moderate"
  model:
    name: vit_base_patch16_224  # Base model name stays the same
    img_size: 384  # Model-specific resolution parameter
  criterion:
    type: "cross_entropy"
    use_class_weights: True
    class_weights: [1.0, 13.0]  # Weight for positive class
  optimizer:
    lr: 0.00004  # Slightly lower LR for higher resolution
  lr_scheduler:
    type: "cosine"  # Cosine annealing scheduler
    warmup:
      enabled: True
      warmup_proportion: 0.1  # Warm up for 10% of first epoch
      min_lr: 1.0e-7  # Starting learning rate for warmup
  optimization:
    gradient_accumulation_steps: 4  # Increase steps for larger images
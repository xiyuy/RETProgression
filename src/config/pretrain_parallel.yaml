# Base configuration for all experiments
distributed: 
  world_size: 1 # Maximum number of GPUs to use
  backend: nccl # Backend to use for distributed training

exp: 
  num_epochs: 1 # Increased from 5 to 8 epochs for better convergence
  checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_GPU1_experiment"
  checkpoint_name: null # "epoch_7.pth"
  checkpoint_freq: 1 # Save checkpoints every N epochs
  experiment_name: "baseline" # Experiment identifier (change for each experiment)
  visualization_dir: "/home/yang.xiyu/Github/RETProgression/src/visualizations" # Directory for plots

data: 
  data_dir: "/projects/retprogression/xiyu_yang/"
  annotations_file_name: "MTM_img_grades_part1_"
  class_labels: "MTM_binary_DR"
  batch_size: 32
  shuffle: True
  num_workers: 7
  resolution: 224  # New parameter for image resolution (default was 224)
  augmentation:
    enabled: True
    strength: "moderate" # Options: "none", "moderate", "strong" 
    visualize: True # Whether to save augmentation visualizations

caching: 
  enabled: True
  train_cache_size: 1000
  val_cache_size: 1000
  prefetch_factor: 2 # Increased prefetch factor

model: 
  name: vit_base_patch16_224
  num_classes: 2
  pretrained: True

criterion: 
  type: "cross_entropy" # Options: "cross_entropy", "focal_loss"
  use_class_weights: True # Whether to use class weighting (for CrossEntropy)
  class_weights: [1.0, 13.0] # Class weights for [negative, positive]
  # Focal Loss parameters
  alpha: 0.25 # Alpha parameter for focal loss (higher values focus more on rare class)
  gamma: 2.0 # Gamma parameter for focal loss (higher values focus more on hard examples)

optimizer: 
  name: adam
  lr: 0.00005 # Learning rate
  weight_decay: 0.001
  momentum: 0.9

lr_scheduler: 
  type: "step" # Options: "step", "cosine"
  step_size: 7
  gamma: 0.1
  warmup:
    enabled: False # Whether to use learning rate warmup
    warmup_proportion: 0.1 # Proportion of first epoch for warmup
    min_lr: 1.0e-7 # Starting learning rate for warmup

optimization: 
  amp_enabled: True # Enable automatic mixed precision
  gradient_clipping: 1.0 # Max gradient norm for gradient clipping
  gradient_accumulation_steps: 2 # Accumulate gradients to reduce communication
  sync_bn: False # Synchronize batch normalization stats across GPUs
  drop_last: True # Drop incomplete batches to avoid small batch issues

# Early stopping configuration
early_stopping: 
  enabled: True
  patience: 5 # Number of epochs with no improvement after which training will be stopped
  metric: balanced_accuracy # Metric to monitor for early stopping
  mode: max # Whether to maximize or minimize the monitored metric
  min_delta: 0.001 # Minimum change to qualify as an improvement

# Profiling disabled to avoid PyTorch internal error
profiling: 
  enabled: False

# Experiment-specific configurations
# ===================================

# Experiment 1: Moderate Augmentation + Focal Loss
experiment_focal_loss:
  exp:
    experiment_name: "focal_loss"
    checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_GPU1_Iteration3_focal_loss"
  data:
    augmentation:
      strength: "moderate"
  criterion:
    type: "focal_loss"
    use_class_weights: False # Not needed for Focal Loss
    alpha: 0.25
    gamma: 2.0
  lr_scheduler:
    warmup:
      enabled: False

# Experiment 2: Moderate Augmentation + Learning Rate Warmup
experiment_warmup: 
  exp:
    num_epochs: 15  # Default: 8
    experiment_name: "warmup"
    checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_GPU1_Iter4_balanced_warmup_cosine"
  data:
    augmentation:
      strength: "moderate"
  criterion:
    type: "cross_entropy"
    use_class_weights: True
  lr_scheduler:
    type: "cosine"  # Default: "step"
    warmup:
      enabled: True
      warmup_proportion: 0.1
      min_lr: 1.0e-7

# Experiment 3: Moderate Augmentation + Focal Loss + Learning Rate Warmup
experiment_focal_warmup:
  exp:
    num_epochs: 15  # Default: 8
    experiment_name: "focal_warmup"
    checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_GPU1_Iteration3_focal_warmup"
  data:
    augmentation:
      strength: "moderate"
  criterion:
    type: "focal_loss"
    use_class_weights: False
    alpha: 1.0
    gamma: 2.0
  lr_scheduler:
    warmup:
      enabled: True
      warmup_proportion: 0.1
      min_lr: 1.0e-7

# Experiment 4: Higher Resolution with Warmup
experiment_high_resolution:
  exp:
    experiment_name: "high_resolution_warmup"
    checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_GPU2_Iteration5_high_resolution_warmup"
    num_epochs: 1  # default is 15.
  data:
    resolution: 384  # Higher resolution for this experiment
    augmentation:
      strength: "moderate"
  model:
    name: vit_base_patch16_224  # Base model name stays the same
    img_size: 384  # Model-specific resolution parameter
  criterion:
    type: "cross_entropy"
    use_class_weights: True
    class_weights: [1.0, 13.0]  # Weight for positive class
  optimizer:
    lr: 0.00004  # Slightly lower LR for higher resolution
  lr_scheduler:
    type: "cosine"  # Cosine annealing scheduler
    warmup:
      enabled: True
      warmup_proportion: 0.1  # Warm up for 10% of first epoch
      min_lr: 1.0e-7  # Starting learning rate for warmup
  optimization:
    gradient_accumulation_steps: 4  # Increase steps for larger images

# Experiment 5: Higher Resolution (from 448 to 1000)
experiment_res_448:
  distributed: 
    world_size: 1 # Maximum number of GPUs to use
  exp:
    experiment_name: "resolution_448_warmup"
    checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_res_448_warmup"
    num_epochs: 10
  data:
    resolution: 448
    batch_size: 32  # Reduced from 64 (baseline) or from 48 (if using 384)
  model:
    name: vit_base_patch16_224
    img_size: 448  # Match with data resolution
  optimizer:
    lr: 0.00004  # Slightly reduced from 0.00005
  lr_scheduler:
    type: "cosine"  # Use cosine annealing 
    warmup:
      enabled: True
      warmup_proportion: 0.1  # Warm up for 10% of first epoch
      min_lr: 1.0e-7
  optimization:
    gradient_accumulation_steps: 2
    amp_enabled: True


experiment_res_512:
  exp:
    experiment_name: "resolution_512_warmup"
    checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_res_512_warmup"
    num_epochs: 10
  data:
    resolution: 512
    batch_size: 24  # Further reduced
  model:
    name: vit_base_patch16_224
    img_size: 512
  optimizer:
    lr: 0.00003
  lr_scheduler:
    type: "cosine"
    warmup:
      enabled: True
      warmup_proportion: 0.1
      min_lr: 1.0e-7
  optimization:
    gradient_accumulation_steps: 3
    amp_enabled: True


experiment_res_640:
  exp:
    experiment_name: "resolution_640_warmup"
    checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_res_640_warmup"
    num_epochs: 10
  data:
    resolution: 640
    batch_size: 16  # Significantly reduced
  model:
    name: vit_base_patch16_224
    img_size: 640
  optimizer:
    lr: 0.00002  # Reduced further
  lr_scheduler:
    type: "cosine"
    warmup:
      enabled: True
      warmup_proportion: 0.1
      min_lr: 1.0e-7
  optimization:
    gradient_accumulation_steps: 4
    amp_enabled: True


experiment_res_768:
  exp:
    experiment_name: "resolution_768_warmup"
    checkpoint_dir: "/home/yang.xiyu/Github/RETProgression/src/checkpoints/referable_res_768_warmup"
    num_epochs: 10
  data:
    resolution: 768
    batch_size: 8  # Dramatically reduced
  model:
    name: vit_base_patch16_224
    img_size: 768
  optimizer:
    lr: 0.00001  # Substantially reduced
  lr_scheduler:
    type: "cosine"
    warmup:
      enabled: True
      warmup_proportion: 0.1
      min_lr: 1.0e-7
  optimization:
    gradient_accumulation_steps: 8
    amp_enabled: True
    gradient_clipping: 1.0  # Maintain gradient clipping


experiment_res_896:
  exp:
    experiment_name: "resolution_896_warmup"
    checkpoint_dir: "/projects/retprogression/xiyu_yang/checkpoints/referable_res_896_warmup"
    num_epochs: 10
  data:
    resolution: 896
    batch_size: 6  # Very small batch size
    augmentation:
      strength: "strong"
  model:
    name: vit_base_patch16_224
    img_size: 896
  optimizer:
    lr: 0.000008  # Very low learning rate
  lr_scheduler:
    type: "cosine"
    warmup:
      enabled: True
      warmup_proportion: 0.1
      min_lr: 1.0e-7
  optimization:
    gradient_accumulation_steps: 12
    amp_enabled: True
    gradient_clipping: 1.0


experiment_res_1000:
  exp:
    experiment_name: "resolution_1000_warmup"
    checkpoint_dir: "/projects/retprogression/xiyu_yang/checkpoints/referable_res_1000_warmup"
    num_epochs: 10
  data:
    resolution: 1000
    batch_size: 4  # Minimum viable batch size
    augmentation:
      strength: "strong"
  model:
    name: vit_base_patch16_224
    img_size: 1000
  optimizer:
    lr: 0.000005  # Extremely low learning rate
  lr_scheduler:
    type: "cosine"
    warmup:
      enabled: True
      warmup_proportion: 0.15  # Slightly longer warmup for highest resolution
      min_lr: 1.0e-7
  optimization:
    gradient_accumulation_steps: 16  # High accumulation to compensate for small batches
    amp_enabled: True
    gradient_clipping: 0.5  # Reduced clipping threshold